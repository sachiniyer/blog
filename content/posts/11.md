---
title: "Efficient Neural Networks"
date: 2024-05-22T02:30:50-04:00
draft: false
---

# Update Oct 2024
UC Berkeley has a AI systems, taught in various versions by [Ion Stoica](https://ucbrise.github.io/cs294-ai-sys-sp19/#today), [Joseph Gonzalez](https://ucbsky.github.io/aisys-fa2024/), [Matei Zaharia](https://learning-systems.notion.site/AI-Systems-LLM-Edition-294-162-Fall-2023-661887583bd340fa851e6a8da8e29abb) (what a cast :astonished:). It's basically just a paper reading course that is updated each iteration. I highly recommend starting there - they do a much better job than me.

# Disclaimer

There are better resources than this if you are trying to learn more about efficient neural networks. 
- https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey (more than just llms)
- https://mlsys.stanford.edu/cs528/
- https://pll.harvard.edu/course/fundamentals-tinyml and https://tinyml.seas.harvard.edu/
- [Matei Zaharia's AI-Systems  Course](https://learning-systems.notion.site/learning-systems/AI-Systems-LLM-Edition-294-162-Fall-2023-661887583bd340fa851e6a8da8e29abb)
- https://github.com/mosharaf/eecs598/tree/w24-genai and https://github.com/mosharaf/eecs598/tree/w21-ai
- https://sites.google.com/view/efficientml/home?authuser=0
- https://hanlab.mit.edu/courses/2023-fall-65940
- https://dlsyscourse.org/
- https://www.fast.ai/ (meh, but has some okay content)

# Papers
The idea is to log some classical papers and some papers that are newer for each category. 

I aim to keep updating this list as I read more.
- Quantization and Dynamic Quantization methods 
  - [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/abs/1511.00363) and [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830)
  - [Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution](http://proceedings.mlr.press/v139/zhang21r.html) 
  - [DNQ: Dynamic Network Quantization](https://arxiv.org/abs/1812.02375) 
  - [CPT: Efficient Deep Neural Network Training via Cyclic Precision](https://openreview.net/forum?id=87ZwsaQNHPZ)
- Pruning of neurons and bits 
  - [Optimal Brain Damage](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html)
  - [Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models](https://arxiv.org/abs/2310.13191)
  - [EDP: An Efficient Decomposition and Pruning Scheme for Convolutional Neural Network Compression](https://ieeexplore.ieee.org/abstract/document/9246734)
- Knowledge Distillation
  - [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
  - [Knowledge Distillation as Semiparametric Inference](https://openreview.net/forum?id=m4UCf24r0Y)
  - [Comparative Knowledge Distillation](https://arxiv.org/abs/2311.02253)
- Federated Learning 
  - [Federated Learning: Strategies for Improving Communication Efficiency](https://arxiv.org/abs/1610.05492) and [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629)
  - [Decentralized Federated Averaging](https://arxiv.org/abs/2104.11375) 
  - [A Field Guide to Federated Optimization](https://arxiv.org/abs/2107.06917) 
  - [Fed-ensemble: Improving Generalization through Model Ensembling in Federated Learning](https://arxiv.org/abs/2107.10663) 
  - [Auxo: Efficient Federated Learning via Scalable Client Clustering](https://arxiv.org/abs/2210.16656) 
- Matrix Operation Optimization (Algorithmic and Hardware) 
  - [Improving the speed of neural networks on CPUs](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37631.pdf)
  - [Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models](https://openreview.net/forum?id=Nfl-iXa-y7R) and [Monarch: Expressive Structured Matrices for Efficient and Accurate Training](https://arxiv.org/abs/2204.00595) 
  - [Implementing block-sparse matrix multiplication kernels using Triton](https://openreview.net/pdf?id=doa11nN5vG)
  - [Sputnik](https://github.com/google-research/sputnik) (not a paper but super cool library to reimplement) 
  - [Efficient Block Approximate Matrix Multiplication](https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ESA.2023.103) and [Fixed-sparsity matrix approximation from matrix-vector products](https://arxiv.org/abs/2402.09379) 
- Model Compression
  - [DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks](https://proceedings.mlr.press/v162/fu22c.html)
